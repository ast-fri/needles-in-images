<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Finding Needles in Images: Can Multi-modal LLMs Locate Fine Details?</title>
  <meta name="description" content="A showcase of the NiM-Benchmark and Spot-IT framework for assessing MLLMs on fine-grained document understanding tasks.">
  <meta name="keywords" content="MLLM, computer vision, document VQA, NiM-Benchmark, Spot-IT, AI research, LLMs, DocVQA">
  <meta name="author" content="Parth Thakkar, Ankush Agarwal, Prasad Kasu, Pulkit Bansal, Chaitanya Devaguptapu">
  <style>
    :root {
      --primary-bg: #e9ecf3;
      --container-bg: #f8fafd;
      --fancy-shadow: 0 12px 36px 0 rgba(27,50,104,0.09);
      --section-shadow: 0 4px 24px 0 rgba(45,78,122,0.07), 0 1.5px 5px 0 #d8dde7;
      --header-main: #21263b;
      --accent: linear-gradient(90deg, #6289ff 0%, #7a6ff0 100%);
      --border: #dbe3ec;
      --badge-bg: #20244c;
      --author-bg: #f0f1f8;
      --author-highlight: #7a6ff0;
      --gradient-light: linear-gradient(120deg, #fafbfc 40%, #eaf1fb 100%);
      --subtle: #7a90b8;
      --section-bg: #f2f4f9;
      --blue-table: #edf2fe;
    }
    body {
      margin: 0;
      background: var(--primary-bg);
      font-family: 'Segoe UI', 'Montserrat', Tahoma, Geneva, Verdana, sans-serif;
      color: #2d3651;
      line-height: 1.8;
    }
    .container {
      max-width: 1100px;
      margin: 42px auto;
      padding: 37px 24px 44px 24px;
      background: var(--container-bg);
      border-radius: 22px;
      box-shadow: var(--fancy-shadow);
    }
    .header {
      text-align: center;
      margin-bottom: 28px;
      padding-top: 6px;
    }
    .header h1 {
      font-size: 2.85rem;
      font-weight: 800;
      letter-spacing: -1.2px;
      margin: 2px 0 7px;
      color: var(--header-main);
      line-height: 1.1;
      background: var(--accent);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
    }
    .header h2 {
      font-size: 1.13rem;
      font-weight: 500;
      color: #314a87;
      margin-bottom: 22px;
      letter-spacing: 0.03em;
      opacity: 0.84;
    }
    .author-list-formal {
      margin: 9px 0 6px 0;
      display: flex;
      flex-direction: column;
      align-items: center;
      gap: 0;
      font-family: 'Times New Roman', serif;
      color: #1d2b41;
      font-weight: 400;
      font-size: 1.09rem;
    }
    .author-names {
      background: var(--author-bg);
      padding: 9px 19px;
      border-radius: 7.5px;
      box-shadow: 0 2.3px 9px #e2e6f3;
      font-size: 1.12rem;
      font-weight: 500;
      margin-bottom: 2px;
    }
    .author-names .sup {
      font-size: 0.93em;
      color: var(--author-highlight);
      vertical-align: super;
    }
    .affiliations {
      font-size: 1.02rem;
      color: #5e6375;
      background: none;
      margin-bottom: 1.5px;
      text-align: center;
    }
    .author-footnote {
      font-size: 0.94rem;
      color: #737cab;
      margin: 0 0 13px;
      text-align: center;
    }
    .links-section {
      display: flex;
      justify-content: center;
      gap: 16px;
      margin: 12px 0 34px 0;
      flex-wrap: wrap;
    }
    .link-button {
      display: inline-flex;
      align-items: center;
      gap: 7px;
      background: var(--badge-bg);
      color: #fff;
      text-decoration: none;
      padding: 10px 22px;
      border-radius: 26px;
      font-size: 1.05rem;
      font-weight: 600;
      transition: background 0.17s;
      box-shadow: 0 2px 8px rgba(34,57,123,0.12);
      background-image: linear-gradient(105deg, #4251a3 0%, #6253e6 100%);
    }
    .link-button.github { background-image: linear-gradient(90deg, #24292e 0%, #3c444a 100%); }
    .link-button.huggingface { background: #ff9900; }
    .link-button.arxiv { background: #b91c1c; }
    .link-button:hover { background: #222a33; }
    .table-of-contents {
      background: var(--gradient-light);
      border: 1.6px solid var(--border);
      border-radius: 14px;
      padding: 22px 18px 20px 24px;
      margin-bottom: 38px;
      box-shadow: 0 1.2px 7px #e0e8f7;
    }
    .table-of-contents h3 {
      margin-top: 0;
      font-size: 1.14rem;
      color: #4e58a2;
      text-transform: uppercase;
      letter-spacing: 0.055em;
    }
    .table-of-contents ul {
      list-style: none;
      padding-left: 0;
      margin: 0;
    }
    .table-of-contents li {
      margin: 7.5px 0;
      font-size: 1.03rem;
    }
    .table-of-contents a {
      color: #2877c3;
      text-decoration: none;
      font-weight: 600;
      border-bottom: 1.2px dotted #4b67e0;
    }
    .table-of-contents a:hover {
      text-decoration: underline;
      color: #362cff;
    }
    .section {
      background: var(--section-bg);
      border: 1.2px solid #e3e8ee;
      border-radius: 13px;
      padding: 35px 31px 24px 31px;
      margin-bottom: 44px;
      box-shadow: var(--section-shadow);
    }
    .section-title {
      font-size: 1.44rem;
      font-weight: 800;
      background: var(--accent);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      margin-bottom: 20px;
      border-bottom: 1.8px solid #d7d8e7;
      display: block;
      padding-bottom: 11px;
      font-family: 'Montserrat', 'Segoe UI', sans-serif;
      letter-spacing: 0.01em;
    }
    p, li, ul {
      font-size: 1.07rem;
      color: #243657;
      margin-bottom: 7px;
    }
    .abstract {
      font-size: 1.11rem;
      color: #212237;
      text-align:justify;
      background: #f9fafb;
      border-radius: 7px;
      padding: 12px 13px;
      margin-bottom: 0;
      box-shadow: 0 1.6px 9px #eef1f9;
    }
    .highlight-box {
      background: linear-gradient(115deg, #e6eeff 40%, #e2f6fb 100%);
      color: #193158;
      border-radius: 12px;
      padding: 18px 22px 15px 18px;
      border-left: 6px solid #7a6ff0;
      margin: 25px 2px 23px 2px;
      box-shadow: 0 1px 5px #e3e6f4;
    }
    .highlight-box h3 {
      margin: 0 0 8px 0;
      color: #2a30a1;
      font-size: 1.07rem;
      font-weight: 700;
    }
    .example-showcase {
      display: flex;
      flex-direction: column;
      gap: 40px;
      margin: 25px 0;
    }
    .example-item {
      display: flex;
      gap: 30px;
      align-items: flex-start;
      background: linear-gradient(135deg, #f8fafb 0%, #f1f5f9 100%);
      border-radius: 16px;
      padding: 25px;
      box-shadow: 0 6px 24px rgba(45,78,122,0.08);
      border: 1px solid #e5eaf2;
    }
    .example-item:nth-child(even) {
      flex-direction: row-reverse;
    }
    .example-item img {
      flex: 1;
      max-width: 500px;
      min-width: 300px;
      border-radius: 12px;
      box-shadow: 0 8px 32px rgba(0,38,90,0.12);
      background: #fff;
      border: 2px solid #dde6f4;
      transition: transform 0.2s, box-shadow 0.2s;
    }
    .example-item img:hover {
      transform: scale(1.02);
      box-shadow: 0 12px 48px rgba(0,38,90,0.18);
    }
    .example-description {
      flex: 1;
      min-width: 280px;
      padding: 10px 15px;
    }
    .example-description h4 {
      font-size: 1.35rem;
      font-weight: 700;
      color: #2d4a87;
      margin: 0 0 15px 0;
      background: linear-gradient(90deg, #6289ff 0%, #7a6ff0 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
    }
    .example-description p {
      font-size: 1.08rem;
      color: #3a4663;
      line-height: 1.7;
      margin: 0;
      text-align: justify;
    }
    .results-table {
      width: 100%;
      border-collapse: collapse;
      margin: 17px 0 10px 0;
      background: var(--blue-table);
      border-radius: 8px;
      overflow: hidden;
      font-size: 1.05rem;
    }
    .results-table th, .results-table td {
      border: 1.2px solid #dde4f7;
      padding: 11px 19px;
      text-align: left;
    }
    .results-table th {
      background: #e3eafc;
      font-weight: 750;
      color: #3753b5;
    }
    .results-table tr:nth-child(even) { background: #f6f8fc; }
    .footer {
      text-align: center;
      font-size: 1rem;
      color: #a9abc5;
      margin-top: 45px;
      padding: 23px 0 8px 0;
      background: transparent;
    }
    @media (max-width: 850px) {
      .container { padding: 8px 3px 17px 7px; }
      .section { padding: 16px 6px 13px 8px; }
      .table-of-contents { padding: 17px 7px 16px 15px; }
      .header h1 { font-size: 1.52rem;}
      .results-table th, .results-table td { padding: 8px 12px; }
      .example-item {
        flex-direction: column !important;
        gap: 20px;
        padding: 20px 15px;
      }
      .example-item img {
        max-width: 100%;
        min-width: unset;
      }
      .example-description {
        min-width: unset;
        padding: 5px 0;
      }
    }
  </style>
</head>
<body>
  <div class="container">
    <header class="header">
      <h1>Finding Needles in Images: Can Multi-modal LLMs Locate Fine Details?</h1>
      <h2>A showcase of the NiM-Benchmark and Spot-IT for fine-grained document AI</h2>
      <div class="author-list-formal">
        <div class="author-names">
          Parth Thakkar<span class="sup">1,&#42;</span>,
          Ankush Agarwal<span class="sup">1,&#42;</span>,
          Prasad Kasu<span class="sup">1</span>,
          Pulkit Bansal<span class="sup">1,2</span>,
          Chaitanya Devaguptapu<span class="sup">1</span>
        </div>
        <div class="affiliations">
          <span class="sup">1</span>Fujitsu Research India &nbsp;&nbsp;
          <span class="sup">2</span>Indian Institute of Technology Patna
        </div>
        <div class="author-footnote">
          <span class="sup">&#42;</span>Equal contribution &nbsp;&nbsp;
        </div>
      </div>
      <div class="links-section">
        <a href="https://github.com/ast-fri/needles-in-images" class="link-button github"><span>ðŸ“„</span> GitHub</a>
        <a href="https://huggingface.co/datasets/AST-FRI/needles-in-images" class="link-button huggingface"><span>ðŸ¤—</span> Hugging Face</a>
        <a href="#" class="link-button arxiv"><span>ðŸ“š</span> arXiv (Coming Soon)</a>
      </div>
    </header>

    <div class="table-of-contents">
      <h3>Table of Contents</h3>
      <ul>
        <li><a href="#abstract">Abstract</a></li>
        <li><a href="#introduction">Introduction</a></li>
        <li><a href="#nim-benchmark">NiM-Benchmark</a></li>
        <li><a href="#spot-it-method">Spot-IT Method</a></li>
        <li><a href="#examples">Qualitative Examples</a></li>
        <li><a href="#results">Results & Analysis</a></li>
        <li><a href="#implementation">Implementation Details</a></li>
        <li><a href="#authors">Authors</a></li>
        <li><a href="#citation">How to Cite</a></li>
      </ul>
    </div>

    <section class="section" id="abstract">
      <span class="section-title">Abstract</span>
      <p class="abstract">
        Multi-modal Large Language Models (MLLMs) can read entire documents, but can they find hidden, precise details buried in complex layouts? <strong>NiM-Benchmark</strong> introduces tough new VQA challenges: fine-grained, localized visual reasoning, like spotting menu prices or a tiny table insight on a research paper. <strong>Spot-IT</strong> delivers an intuitive approach that helps MLLMs "zoom in" â€” intelligently picking image patches and applying a dynamic Gaussian focus guided by the user's query. Large-scale evaluation sets new accuracy records and reveals just how far these models still lag behind human readers, especially for tiny, context-heavy details.
      </p>
    </section>

    <section class="section" id="introduction">
      <span class="section-title">Introduction</span>
      <p>
        Vision-Language Models are bridging the gap between seeing and understanding, but their real-world utility hinges on precision: can they find <em>just</em> the answerâ€”no more, no lessâ€”when it occupies only a sliver of a dense document? Most DocVQA benchmarks miss this, focusing on broad summaries or generic extraction, not the real-world pain-point of hidden, context-rich queries.
      </p>
      <div class="highlight-box">
        <h3>Why does this matter?</h3>
        Visual document QA fails if models can't locate tiny, vital detailsâ€”just as users do when flipping desperately through legal contracts, academic papers, or digital receipts!
      </div>
    </section>

    <section class="section" id="nim-benchmark">
      <span class="section-title">NiM-Benchmark: Fine-Grained Challenges</span>
      <p>
        <b>NiM-Benchmark</b> is crafted for precision stress-testing, with <b>2,970 images</b> and <b>1,180 question-answer</b> pairs, from everyday sources:
      </p>
      <ul>
        <li>Restaurant menus (prices, dish attributes)</li>
        <li>Academic papers (tiny figure/table details, technical params)</li>
        <li>Magazines, newspapers</li>
        <li>Website & lecture screenshots</li>
      </ul>
      <p>
        Each question targets a region covering <5% of the visual area. Domain variety and question complexity is ensured (see paper Table 9).
      </p>
      <table class="results-table">
        <tr>
          <th>Domain</th>
          <th>Sample Types</th>
        </tr>
        <tr>
          <td>Restaurant Menus</td>
          <td>Prices, dish ingredients, dietary info</td>
        </tr>
        <tr>
          <td>Academic Papers</td>
          <td>Figure/table values, technical parameters</td>
        </tr>
        <tr>
          <td>Magazines / Newspapers</td>
          <td>Date, events, entities, scores</td>
        </tr>
        <tr>
          <td>Websites / Lectures</td>
          <td>UI details, content, nav elements</td>
        </tr>
      </table>
    </section>

    <section class="section" id="spot-it-method">
      <span class="section-title">Spot-IT: Human-Like Visual Focus for MLLMs</span>
      <div style="display: flex; gap: 24px; flex-wrap: wrap; margin: 20px 0;">
        <div style="flex: 1 1 280px; background: #eef1fa; border-radius: 8px; padding: 15px 17px 6px 17px; margin-bottom: 9px; box-shadow: 0 1px 7px #e0ebfa;">
          <h4 style="margin-top:0;font-size:1.11rem;margin-bottom:7px;">1. Query-Guided Patch Identification</h4>
          <ul>
            <li>Divide image into n Ã— n patches (n=6),</li>
            <li>Vision-language model (SigLip) matches patches with the query (cosine similarity),</li>
            <li>Pick patch with highest similarity for focus,</li>
          </ul>
        </div>
        <div style="flex: 1 1 280px; background: #eef1fa; border-radius: 8px; padding: 15px 17px 6px 17px; margin-bottom: 9px; box-shadow: 0 1px 7px #e0ebfa;">
          <h4 style="margin-top:0;font-size:1.11rem;margin-bottom:7px;">2. Dynamic Gaussian Attention</h4>
          <ul>
            <li>Draw a Gaussian mask centered at patch, width based on confidence,</li>
            <li>Sharper focus for strong matches, softer for weak ones,</li>
            <li>Blend region into original image & send to MLLM,</li>
          </ul>
        </div>
        <div style="flex: 1 1 280px; background: #eef1fa; border-radius: 8px; padding: 15px 17px 6px 17px; margin-bottom: 9px; box-shadow: 0 1px 7px #e0ebfa;">
          <h4 style="margin-top:0;font-size:1.11rem;margin-bottom:7px;">3. Answer Generation</h4>
          <ul>
            <li>Standard DocVQA prompt (with focus area),</li>
            <li>No need to retrain model.</li>
          </ul>
        </div>
      </div>
      <div class="highlight-box">
        <h3>What makes this human like?</h3>
        Spot-IT lets models zoom in like a humanâ€”keeping the page in view but staring at just the vital spot.
      </div>
    </section>

    <section class="section" id="examples">
      <span class="section-title">Qualitative Examples</span>
      <div class="example-showcase">
        <div class="example-item">
          <img src="Sample_imgs/Restaurant.png" alt="NiM Spot-IT: Restaurant Menu">
          <div class="example-description">
            <h4>Restaurant Menu Analysis</h4>
            <p>Spot-IT successfully identifies and focuses on specific menu items and prices within complex restaurant menu layouts. The system can precisely locate fine details like individual dish prices, ingredients, and dietary information that occupy small regions of the overall document.</p>
          </div>
        </div>
        
        <div class="example-item">
          <img src="Sample_imgs/Lecture SS.png" alt="NiM Spot-IT: Lecture Screenshot">
          <div class="example-description">
            <h4>Lecture Screenshot Extraction</h4>
            <p>Academic content presents unique challenges with dense information layouts. Spot-IT demonstrates its ability to extract fine-grained details from lecture slides, focusing on specific data points, formulas, or textual elements that would be difficult to identify without targeted attention mechanisms.</p>
          </div>
        </div>
        
        <div class="example-item">
          <img src="Sample_imgs/Website_SS.png" alt="NiM Spot-IT: Website Screenshot">
          <div class="example-description">
            <h4>Website UI Element Focus</h4>
            <p>Web interfaces contain numerous small interactive elements and textual details. This example showcases how Spot-IT can pinpoint specific UI components, navigation elements, or content sections within complex website layouts, enabling precise information extraction from digital interfaces.</p>
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="results">
      <span class="section-title">Results & Analysis</span>
      <p>
        Spot-IT was tested on open-source and leading commercial MLLMs. Highlights:
      </p>
      <ul>
        <li><b>15-21% accuracy improvement</b> on NiM-Benchmark and ArxiVQA, over basic MLLM/OCR DocVQA pipelines.</li>
        <li>Even GPT-4o achieves <b>only 38% EM</b> (humans get 63%), showing how tough fine-grained questions are.</li>
        <li>Most failures: model hallucination, missing tiny evidence, or reasoning breakdown.</li>
        <li>Latency only +4sâ€”acceptable for detailed search.</li>
      </ul>
      <table class="results-table">
        <tr>
          <th>Model</th>
          <th>NiM EM (%)</th>
          <th>NiM F1 (%)</th>
          <th>Human EM (%)</th>
        </tr>
        <tr>
          <td>GPT-4o</td>
          <td>38</td>
          <td>48</td>
          <td rowspan="3" style="background:#f9fcfb;">63</td>
        </tr>
        <tr>
          <td>GPT-4o + Spot-IT</td>
          <td><b>46</b></td>
          <td><b>56</b></td>
        </tr>
        <tr>
          <td>Gemini-1.5-Flash</td>
          <td>22</td>
          <td>28</td>
        </tr>
      </table>
      <div class="highlight-box">
        <b>Bottom line:</b> Spot-IT is a step ahead for fine-grained DocVQA, but there's lots of room to match humans!
      </div>
    </section>

    <section class="section" id="implementation">
      <span class="section-title">Implementation Details</span>
      <ul>
        <li>Dataset : <b>Hugging Face</b> (see top links).</li>
        <li>Spot-IT is a plug-in stepâ€”no need to retrain! Works across modern MLLMs.</li>
        <li>Experiments done on NVIDIA A30 GPUs and with all major API-access models.</li>
      </ul>
    </section>

    <section class="section" id="authors">
      <span class="section-title">Authors & Affiliations</span>
      <div class="author-list-formal">
        <div class="author-names">
          Parth Thakkar<span class="sup">1,&#42;</span>,
          Ankush Agarwal<span class="sup">1,&#42;</span>,
          Prasad Kasu<span class="sup">1</span>,
          Pulkit Bansal<span class="sup">1,2,â€ </span>,
          Chaitanya Devaguptapu<span class="sup">1</span>
        </div>
        <div class="affiliations">
          <span class="sup">1</span>Fujitsu Research India &nbsp;&nbsp;
          <span class="sup">2</span>Indian Institute of Technology Patna
        </div>
        <div class="author-footnote">
          <span class="sup">&#42;</span>Equal contribution &nbsp;&nbsp;
          <span class="sup">â€ </span>Work partially at Fujitsu Research India
        </div>
      </div>
    </section>

    <section class="section" id="citation">
      <span class="section-title">How to Cite</span>
<pre style="background:#e5e9f2; border:1px solid #cfd9ed;border-radius:9px;padding:11px 8px;font-size:1.04rem;overflow-x:auto; color:#232735;">
@inproceedings{thakkar2024findingneedles,
  title = {Finding Needles in Images: Can Multimodal LLMs Locate Fine Details?},
  author = {Parth Thakkar and Ankush Agarwal and Prasad Kasu and Pulkit Bansal and Chaitanya Devaguptapu},
  booktitle = {ACL 2025},
  year = {2025}
}
</pre>
    </section>
    <div class="footer">
      Â© 2025 Research Project | Finding Needles in Images (NiM-Benchmark & Spot-IT)
    </div>
  </div>
</body>
</html>