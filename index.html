<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Finding Needles in Images: Can Multi-modal LLMs Locate Fine Details?</title>
  <meta name="description" content="A professional overview of the NiM-Benchmark and Spot-IT method for evaluating Multi-modal LLMs on precise document understanding tasks.">
  <meta name="keywords" content="MLLM, computer vision, document VQA, NiM-Benchmark, Spot-IT, fine-grained, AI research">
  <meta name="author" content="Parth Thakkar, Ankush Agarwal, Prasad Kasu, Pulkit Bansal, Chaitanya Devaguptapu">
  <style>
    body { margin: 0; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; background: #f6f8fa; color: #222; line-height: 1.7; }
    .container { max-width: 1050px; margin: 0 auto; padding: 50px 25px; }
    .header { text-align: center; margin-bottom: 38px; }
    .header h1 { font-size: 2.6rem; font-weight: 700; color: #263245; letter-spacing: -1px; margin-bottom: 10px; }
    .header h2 { font-size: 1.1rem; font-weight: 400; color: #556177; margin-bottom: 25px; }
    .author-list { margin: 18px 0 10px 0; display: flex; justify-content: center; flex-wrap: wrap; gap: 25px; }
    .author { background: #ecf2fb; border-radius: 7px; padding: 13px 26px; font-size: 1rem; color: #022e57; }
    .links-section { display: flex; justify-content: center; gap: 16px; margin: 20px 0 40px 0; flex-wrap: wrap; }
    .link-button { display: inline-flex; align-items: center; gap: 9px; background: #283b5b; color: #fff; text-decoration: none; padding: 10px 24px; border-radius: 26px; font-size: 0.97rem; font-weight: 500; transition: background 0.19s; }
    .link-button.github { background: #23272e; }
    .link-button.huggingface { background: #ff9900; }
    .link-button.arxiv { background: #a21c2e; }
    .link-button:hover { background: #0e4775; }
    .section { background: #fff; border: 1px solid #e3e8ee; border-radius: 10px; padding: 35px 31px 28px 31px; margin-bottom: 40px; box-shadow: 0 2px 11px rgba(35,51,75,0.02); }
    .section-title { font-size: 1.4rem; font-weight: 600; margin-bottom: 17px; border-bottom: 1px solid #e5e5e9; display: block; color: #152235; padding-bottom: 9px; }
    .abstract { font-size: 1.06rem; color: #222; text-align:justify; }
    .image-row { display: flex; flex-wrap: wrap; gap: 23px; justify-content: center; align-items: flex-end; margin-bottom: 0; }
    .image-row img { border-radius: 8px; box-shadow: 0 2px 14px rgba(0,0,0,0.10); max-width: 340px; background: #f6faff;}
    .image-label { text-align: center; color: #64738a; font-size: 0.91rem; margin: 7px 0 0 0; }
    .highlight-box { background: linear-gradient(135deg, #4367c8 0%, #764ba2 100%); color: #fff; border-radius: 10px; padding: 20px; margin: 26px 0; }
    .highlight-box h3 { margin: 0 0 10px 0; color: #fff; font-size: 1.12rem;}
    .results-table { width: 100%; border-collapse: collapse; margin: 16px 0 10px 0; background: #fafdff; }
    .results-table th, .results-table td { border: 1px solid #e0e5ed; padding: 10.5px 14px; }
    .results-table th { background: #edf2fa; font-weight: 650; color: #28375b; }
    .results-table tr:nth-child(even) { background: #f6f8fb; }
    .table-of-contents { background: #f5f7fa; border: 1px solid #e8e9ec; border-radius: 9px; padding: 19px 19px 22px 19px; margin-bottom: 34px; }
    .table-of-contents h3 { margin-top: 0; font-size: 1.12rem; color: #374151;}
    .table-of-contents ul { list-style: none; padding-left: 0; }
    .table-of-contents li { margin: 8.5px 0; }
    .table-of-contents a { color: #2877c3; text-decoration: none; font-weight: 500; }
    .table-of-contents a:hover { text-decoration: underline; }
    .author-role { font-size: 0.91rem; color: #69798c; margin-left: 6px; }
    .footer { text-align: center; font-size: 0.9rem; color: #a1a6ae; margin-top: 55px; padding: 20px 0 10px 0;}
    @media (max-width: 780px) {
      .container { padding: 14px 7px; }
      .section { padding: 28px 7px 17px 7px; }
      .author-list { flex-direction: column; align-items: center; gap: 5px;}
      .image-row { flex-direction: column; }
    }
  </style>
</head>
<body>
  <div class="container">
    <header class="header">
      <h1>Finding Needles in Images: Can Multi-modal LLMs Locate Fine Details?</h1>
      <h2>A professional overview of the NiM-Benchmark and Spot-IT method for fine-grained document understanding</h2>
      <div class="author-list">
        <div class="author">Parth Thakkar<span class="author-role">Fujitsu Research India</span></div>
        <div class="author">Ankush Agarwal<span class="author-role">Fujitsu Research India</span></div>
        <div class="author">Prasad Kasu<span class="author-role">Fujitsu Research India</span></div>
        <div class="author">Pulkit Bansal<span class="author-role">Fujitsu Research India / IIT Patna</span></div>
        <div class="author">Chaitanya Devaguptapu<span class="author-role">Fujitsu Research India</span></div>
      </div>
      <div class="links-section">
        <a href="https://github.com/ast-fri/needles-in-images" class="link-button github"><span>ðŸ“„</span> GitHub</a>
        <a href="https://huggingface.co/datasets/AST-FRI/needles-in-images" class="link-button huggingface"><span>ðŸ¤—</span> Hugging Face</a>
        <a href="#" class="link-button arxiv"><span>ðŸ“š</span> arXiv (Coming Soon)</a>
      </div>
    </header>

    <div class="table-of-contents">
      <h3>Table of Contents</h3>
      <ul>
        <li><a href="#abstract">Abstract</a></li>
        <li><a href="#introduction">Introduction</a></li>
        <li><a href="#nim-benchmark">NiM-Benchmark</a></li>
        <li><a href="#spot-it-method">Spot-IT Method</a></li>
        <li><a href="#examples">Qualitative Examples</a></li>
        <li><a href="#results">Results & Analysis</a></li>
        <li><a href="#implementation">Implementation Details</a></li>
        <li><a href="#authors">Authors</a></li>
        <li><a href="#citation">How to Cite</a></li>
      </ul>
    </div>

    <section class="section" id="abstract">
      <span class="section-title">Abstract</span>
      <p class="abstract">
        Multi-modal Large Language Models (MLLMs) have revolutionized document understanding, but their limitations in identifying and reasoning about fine-grained details within complex layouts are not well understood. The <b>NiM-Benchmark</b> introduces a new challenge: evaluating visual question answering (VQA) on fine, localized details â€” from menu pricing to technical details in academic papers â€” that might occupy less than 5% of a page. We further present <b>Spot-IT</b>, an intuitive approach that empowers MLLMs to better locate and extract such "needles in images" by combining intelligent patch selection with Gaussian-based visual attention, simulating how humans naturally zoom and focus. Experiments show substantial gains in accuracy over common baselines, especially in high-complexity, real-world scenarios.
      </p>
    </section>

    <section class="section" id="introduction">
      <span class="section-title">Introduction</span>
      <p>
        The rapid progress of Multi-modal LLMs has enabled substantial breakthroughs in machine comprehension of complex documents through tasks like Document Visual Question Answering (DocVQA). Yet, the real challenge arises when models must zoom in on small, contextually critical regions â€” for example, identifying a specific menu item's price or a single clause in a contract, requiring attention to just a sliver of the input. Most existing benchmarks are geared toward broad comprehension and miss this nuanced evaluation.
      </p>
      <div class="highlight-box">
        <h3>Core problem:</h3>
        Can MLLMs reliably find and reason about fine-grained, context-rich details â€” not just broad document summaries â€” within complex layouts?
      </div>
    </section>

    <section class="section" id="nim-benchmark">
      <span class="section-title">NiM-Benchmark: Fine-Grained DocVQA for the Real World</span>
      <p>
        <b>NiM-Benchmark</b> is designed to fill this gap. It comprises <b>2,970 images</b> and <b>1,180 question-answer</b> pairs spanning six realistic document domains:
      </p>
      <ul>
        <li>Restaurant menus</li>
        <li>Academic papers</li>
        <li>Magazines</li>
        <li>Newspapers</li>
        <li>Website screenshots</li>
        <li>Lecture screenshots</li>
      </ul>
      <p>
        Each question tests the model's ability to extract extremely localized and context-sensitive information (fine-grained details defined as &lt;5% of page area). The dataset features a spectrum of reasoning: from inline fact extraction, boolean checks, and comparative questions to more complex multi-step logical inference (see Table 9 in the original paper).
      </p>
      <table class="results-table">
        <tr>
          <th>Domain</th>
          <th>Sample Types</th>
        </tr>
        <tr>
          <td>Restaurant Menus</td>
          <td>Prices, dish ingredients, dietary info</td>
        </tr>
        <tr>
          <td>Academic Papers</td>
          <td>Figure/table values, technical parameters</td>
        </tr>
        <tr>
          <td>Magazines / Newspapers</td>
          <td>Date, event, named entity mentions, box scores</td>
        </tr>
        <tr>
          <td>Websites / Lectures</td>
          <td>UI details, classroom content, navigation elements</td>
        </tr>
      </table>
    </section>

    <section class="section" id="spot-it-method">
      <span class="section-title">Spot-IT: Human-Inspired Fine-Grained Attention for MLLMs</span>
      <div class="methodology-grid">
        <div class="method-card">
          <h4>1. Query-Guided Patch Identification</h4>
          <ul>
            <li>Each image is divided into a grid of patches (n x n), empirically optimized at n=6.</li>
            <li>A vision-language model (SigLip) measures semantic similarity between the user's question and each grid patch.</li>
            <li>The most relevant patch (highest cosine similarity) is chosen as the "region of interest" for deeper inspection.</li>
          </ul>
        </div>
        <div class="method-card">
          <h4>2. Adaptive Gaussian Attention</h4>
          <ul>
            <li>A Gaussian mask is generated, centered on the identified patch; its spread is dynamically tuned by the confidence in the patch selection.</li>
            <li>High-confidence matches yield sharply focused attention, low-confidence gives a broader view (see Eq. 9-11 in the paper).</li>
            <li>The original image, modulated with this attention mask, is then given to the MLLM for answer generation.</li>
          </ul>
        </div>
        <div class="method-card">
          <h4>3. Answer Generation</h4>
          <ul>
            <li>Standard MLLM DocVQA prompt, with specific emphasis on the attended region.</li>
            <li>No additional model retraining is needed; Spot-IT is a zero-shot post-processing augmentation.</li>
          </ul>
        </div>
      </div>
      <div class="highlight-box">
        <h3>Why is this different?</h3>
        Spot-IT targets the dual challenge of global context retention with local feature focus â€” simulating how a human would visually scan and zoom to find "the needle in a haystack" within a document.
      </div>
    </section>

    <section class="section" id="examples">
      <span class="section-title">Qualitative Examples</span>
      <div class="image-row">
        <div>
          <img src="Sample_imgs/Restaurant.png" alt="NiM Spot-IT: Restaurant Menu">
          <div class="image-label">Restaurant menu â€” answer region precisely localized using Spot-IT</div>
        </div>
        <div>
          <img src="Sample_imgs/Lecture SS.png" alt="NiM Spot-IT: Lecture Screenshot">
          <div class="image-label">Lecture screenshot â€” fine-grained detail extraction</div>
        </div>
        <div>
          <img src="Sample_imgs/Website_SS.png" alt="NiM Spot-IT: Website Screenshot">
          <div class="image-label">Website screenshot â€” UI element focus</div>
        </div>
      </div>
    </section>

    <section class="section" id="results">
      <span class="section-title">Results & Analysis</span>
      <p>
        Spot-IT was evaluated against both open-source and leading commercial MLLMs, across multiple benchmarks. Key findings:
      </p>
      <ul>
        <li><b>Spot-IT significantly outperforms</b> basic MLLM and OCR-based DocVQA methods for fine-grained detail tasks, with improvements ranging from 15% to over 20% in key metrics such as EM (Exact Match) and F1 on NiM-Benchmark and ArxiVQA[1].</li>
        <li>The challenge remains highly difficult: even state-of-the-art MLLMs like GPT-4o <b>achieve only 38% EM</b>, vs. humans at 63% EM, underscoring the complexity of these visual reasoning tasks.</li>
        <li>Error analysis indicates that failures are most frequently due to incomplete or hallucinated evidence, as well as perceptual and reasoning errors on fine details.</li>
        <li>Spot-IT incurred a marginal latency penalty (~4 seconds), judged acceptable for substantial accuracy gains.</li>
      </ul>
      <table class="results-table">
        <tr>
          <th>Model</th>
          <th>NiM EM (%)</th>
          <th>NiM F1 (%)</th>
          <th>Human EM (%)</th>
        </tr>
        <tr>
          <td>GPT-4o</td>
          <td>38</td>
          <td>48</td>
          <td rowspan="3" style="background:#f6faf9;">63</td>
        </tr>
        <tr>
          <td>GPT-4o + Spot-IT</td>
          <td><b>46</b></td>
          <td><b>56</b></td>
        </tr>
        <tr>
          <td>Gemini-1.5-Flash</td>
          <td>22</td>
          <td>28</td>
        </tr>
      </table>
      <div class="highlight-box">
        <b>Conclusion:</b> Spot-IT sets a new state-of-the-art for fine-grained DocVQA, but there is still a substantial gap between machine and human detail comprehension, highlighting rich opportunities for future work.
      </div>
    </section>

    <section class="section" id="implementation">
      <span class="section-title">Implementation Details</span>
      <ul>
        <li>All code and dataset are available via <b>GitHub</b> and <b>Hugging Face</b> (see links above).</li>
        <li>Spot-IT does not require re-training of underlying models. It is an efficient, modular post-processing step that can plug into existing MLLM VQA pipelines.</li>
        <li>Experiments conducted on NVIDIA A30 GPUs and with API access to GPT-4o, Gemini, Qwen2, Llama-3.2 and others.</li>
      </ul>
    </section>

    <section class="section" id="authors">
      <span class="section-title">Authors & Affiliations</span>
      <div class="author-list">
        <div class="author">Parth Thakkar<span class="author-role">Fujitsu Research India</span></div>
        <div class="author">Ankush Agarwal<span class="author-role">Fujitsu Research India</span></div>
        <div class="author">Prasad Kasu<span class="author-role">Fujitsu Research India</span></div>
        <div class="author">Pulkit Bansal<span class="author-role">Fujitsu Research India / IIT Patna</span></div>
        <div class="author">Chaitanya Devaguptapu<span class="author-role">Fujitsu Research India</span></div>
      </div>
      <div style="text-align:center; margin-top:10px; color:#4d5868;">
        <i>Equal contribution by Parth Thakkar and Ankush Agarwal. Work partly done at Fujitsu Research India by Pulkit Bansal.</i>
      </div>
    </section>

    <section class="section" id="citation">
      <span class="section-title">How to Cite</span>
<pre style="background:#f5f7fa; border:1px solid #d8dee9;border-radius:7px;padding:13px 9px;font-size:.99rem;overflow-x:auto; color:#232735;">
@inproceedings{thakkar2024findingneedles,
  title = {Finding Needles in Images: Can Multimodal LLMs Locate Fine Details?},
  author = {Parth Thakkar and Ankush Agarwal and Prasad Kasu and Pulkit Bansal and Chaitanya Devaguptapu},
  booktitle = {ACL 2025},
  year = {2025}
}
</pre>
    </section>

    <div class="footer">
      Â© 2025 Research Project | Finding Needles in Images (NiM-Benchmark & Spot-IT)
    </div>
  </div>
</body>
</html>