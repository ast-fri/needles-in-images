<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Finding Needles in Images: Can Multi-modal LLMs Locate Fine Details?</title>
  <meta name="description" content="A comprehensive benchmark and method for evaluating Multi-modal Large Language Models on fine-grained visual document understanding tasks.">
  <meta name="keywords" content="MLLM, computer vision, document understanding, benchmark, AI, machine learning">
  <meta name="author" content="AST-FRI Research Team">
  <style>
    body {
      margin: 0;
      font-family: 'Segoe UI', Tahoma, sans-serif;
      background-color: #fafafa;
      color: #222;
      line-height: 1.7;
    }

    .container {
      max-width: 960px;
      margin: 0 auto;
      padding: 40px 20px;
    }

    .header {
      text-align: center;
      margin-bottom: 40px;
    }

    .header h1 {
      font-size: 2.4rem;
      font-weight: 700;
      color: #000;
      margin-bottom: 12px;
      border-bottom: 2px solid #ccc;
      display: inline-block;
      padding-bottom: 6px;
    }

    .header p {
      font-size: 1rem;
      color: #555;
    }

    .links-section {
      display: flex;
      justify-content: center;
      gap: 15px;
      flex-wrap: wrap;
      margin-top: 25px;
    }

    .link-button {
      display: inline-flex;
      align-items: center;
      gap: 8px;
      background: #222;
      color: #fff;
      text-decoration: none;
      padding: 10px 20px;
      border-radius: 30px;
      font-size: 0.9rem;
      transition: background 0.2s ease;
    }

    .link-button:hover {
      background: #444;
    }

    .link-button.github {
      background: #24292e;
    }

    .link-button.huggingface {
      background: #ff9900;
    }

    .link-button.arxiv {
      background: #b91c1c;
    }

    .section {
      background: #fff;
      border: 1px solid #e0e0e0;
      border-radius: 8px;
      padding: 30px;
      margin-bottom: 40px;
      box-shadow: 0 2px 5px rgba(0, 0, 0, 0.02);
    }

    .section-title {
      font-size: 1.5rem;
      font-weight: 500;
      margin-bottom: 20px;
      border-bottom: 1px solid #ddd;
      padding-bottom: 10px;
      color: #111;
    }

    .abstract {
      font-size: 1rem;
      color: #333;
      text-align: justify;
      margin-bottom: 15px;
    }

    .image-row {
      display: flex;
      flex-wrap: wrap;
      gap: 20px;
      justify-content: center;
      align-items: center;
    }

    .image-row img {
      border-radius: 8px;
      box-shadow: 0 2px 10px rgba(0, 0, 0, 0.05);
      max-width: 100%;
    }

    .image-label {
      text-align: center;
      color: #666;
      font-size: 0.9rem;
      margin-top: 8px;
    }

    .footer {
      text-align: center;
      font-size: 0.9rem;
      color: #888;
      margin-top: 60px;
      padding: 20px;
    }

    .table-of-contents {
      background: #f8f9fa;
      border: 1px solid #e9ecef;
      border-radius: 8px;
      padding: 20px;
      margin-bottom: 30px;
    }

    .table-of-contents h3 {
      margin-top: 0;
      color: #333;
      font-size: 1.2rem;
    }

    .table-of-contents ul {
      list-style: none;
      padding-left: 0;
    }

    .table-of-contents li {
      margin: 8px 0;
    }

    .table-of-contents a {
      color: #007bff;
      text-decoration: none;
      font-weight: 500;
    }

    .table-of-contents a:hover {
      text-decoration: underline;
    }

    .methodology-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 20px;
      margin: 20px 0;
    }

    .method-card {
      background: #f8f9fa;
      border: 1px solid #e9ecef;
      border-radius: 8px;
      padding: 20px;
    }

    .method-card h4 {
      color: #333;
      margin-top: 0;
      font-size: 1.1rem;
    }

    .results-table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
      background: white;
    }

    .results-table th,
    .results-table td {
      border: 1px solid #ddd;
      padding: 12px;
      text-align: left;
    }

    .results-table th {
      background-color: #f8f9fa;
      font-weight: 600;
      color: #333;
    }

    .results-table tr:nth-child(even) {
      background-color: #f9f9f9;
    }

    .highlight-box {
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      color: white;
      padding: 20px;
      border-radius: 8px;
      margin: 20px 0;
    }

    .highlight-box h3 {
      margin-top: 0;
      color: white;
    }

    .code-block {
      background: #f8f9fa;
      border: 1px solid #e9ecef;
      border-radius: 4px;
      padding: 15px;
      font-family: 'Courier New', monospace;
      font-size: 0.9rem;
      overflow-x: auto;
      margin: 15px 0;
    }

    .author-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
      gap: 20px;
      margin: 20px 0;
    }

    .author-card {
      text-align: center;
      padding: 15px;
      background: #f8f9fa;
      border-radius: 8px;
    }

    .comparison-chart {
      background: white;
      border: 1px solid #e0e0e0;
      border-radius: 8px;
      padding: 20px;
      margin: 20px 0;
    }

    @media (max-width: 768px) {
      .image-row {
        flex-direction: column;
      }
      
      .methodology-grid {
        grid-template-columns: 1fr;
      }
      
      .author-grid {
        grid-template-columns: 1fr;
      }
      
      .results-table {
        font-size: 0.8rem;
      }
    }
  </style>
</head>
<body>
  <div class="container">
    <header class="header">
      <h1>Finding Needles in Images: Can Multi-modal LLMs Locate Fine Details?</h1>
      <p>A comprehensive benchmark and method for evaluating Multi-modal Large Language Models on fine-grained visual document understanding tasks.</p>

      <div class="links-section">
        <a href="https://github.com/ast-fri/needles-in-images" class="link-button github">
          <span>üìÑ</span> GitHub
        </a>
        <a href="https://huggingface.co/datasets/AST-FRI/needles-in-images" class="link-button huggingface">
          <span>ü§ó</span> Hugging Face
        </a>
        <a href="https://arxiv.org/abs/your-paper-id" class="link-button arxiv">
          <span>üìö</span> arXiv (Coming Soon)
        </a>
      </div>
    </header>

    <!-- Table of Contents -->
    <div class="table-of-contents">
      <h3>üìã Table of Contents</h3>
      <ul>
        <li><a href="#abstract">Abstract</a></li>
        <li><a href="#introduction">Introduction</a></li>
        <li><a href="#benchmark">NiM-Benchmark</a></li>
        <li><a href="#methodology">Spot-IT Methodology</a></li>
        <li><a href="#examples">Examples</a></li>
        <li><a href="#results">Results & Analysis</a></li>
        <li><a href="#comparison">Comparison with Baselines</a></li>
        <li><a href="#implementation">Implementation Details</a></li>
        <li><a href="#authors">Authors</a></li>
        <li><a href="#citation">Citation</a></li>
      </ul>
    </div>

    <!-- Abstract -->
    <section id="abstract" class="section">
      <div class="section-title">üìÑ Abstract</div>
      <p class="abstract">
        While Multi-modal Large Language Models (MLLMs) have shown impressive capabilities in document understanding tasks, their ability to locate and reason about fine-grained details within complex documents remains understudied. Consider searching a restaurant menu for a specific nutritional detail or locating a particular warranty clause in a manual ‚Äì tasks that require precise attention to minute details within a larger context, akin to Finding Needles in Images (NiM). 
      </p>
      <p class="abstract">
        To address this gap, we introduce <strong>NiM-Benchmark</strong>, a carefully curated benchmark spanning diverse real-world documents including newspapers, menus, and lecture images, specifically designed to evaluate MLLMs' capability in these intricate tasks. Building on this, we further propose <strong>Spot-IT</strong>, a simple yet effective approach that enhances MLLMs capability through intelligent patch selection and Gaussian attention, motivated from how humans zoom and focus when searching documents.
      </p>
      <p class="abstract">
        Our extensive experiments reveal both the capabilities and limitations of current MLLMs in handling fine-grained document understanding tasks, while demonstrating the effectiveness of our approach. Spot-IT achieves significant improvements over baseline methods, particularly in scenarios requiring precise detail extraction from complex layouts.
      </p>
    </section>

    <!-- Introduction -->
    <section id="introduction" class="section">
      <div class="section-title">üéØ Introduction</div>
      <p>
        In our daily lives, we constantly perform tasks that require finding specific information within complex visual documents. Whether it's locating a particular ingredient in a restaurant menu, finding a specific clause in a legal document, or identifying key information in a lecture slide, these tasks demand precise visual attention and reasoning capabilities.
      </p>
      
      <div class="highlight-box">
        <h3>üîç The Challenge</h3>
        <p>Current Multi-modal Large Language Models (MLLMs) excel at high-level document understanding but struggle with fine-grained detail extraction. This limitation becomes particularly evident when dealing with:</p>
        <ul>
          <li><strong>Dense layouts:</strong> Documents with multiple columns, tables, and overlapping elements</li>
          <li><strong>Small text:</strong> Fine print, footnotes, and detailed specifications</li>
          <li><strong>Complex structures:</strong> Nested information hierarchies and non-linear reading patterns</li>
          <li><strong>Visual noise:</strong> Backgrounds, decorative elements, and competing visual information</li>
        </ul>
      </div>

      <p>
        Our research addresses this gap by introducing both a comprehensive evaluation framework and a novel methodology that mimics human visual attention patterns when searching for specific information in complex documents.
      </p>
    </section>

    <!-- Benchmark -->
    <section id="benchmark" class="section">
      <div class="section-title">üìä NiM-Benchmark: A Comprehensive Evaluation Framework</div>
      
      <p>
        NiM-Benchmark is designed to systematically evaluate MLLMs' ability to locate and extract fine-grained information from real-world documents. Our benchmark includes:
      </p>

      <div class="methodology-grid">
        <div class="method-card">
          <h4>üçΩÔ∏è Restaurant Menus</h4>
          <p>Complex layouts with prices, descriptions, nutritional information, and allergen details scattered across multiple sections.</p>
          <ul>
            <li>Multi-column layouts</li>
            <li>Varied typography</li>
            <li>Price-item associations</li>
            <li>Categorical organization</li>
          </ul>
        </div>

        <div class="method-card">
          <h4>üì∞ Newspapers</h4>
          <p>Dense information with headlines, body text, captions, and advertisements requiring precise location skills.</p>
          <ul>
            <li>Multi-story layouts</li>
            <li>Image-text relationships</li>
            <li>Hierarchical information</li>
            <li>Advertisement separation</li>
          </ul>
        </div>

        <div class="method-card">
          <h4>üìö Lecture Slides</h4>
          <p>Educational content with bullet points, diagrams, formulas, and references requiring academic precision.</p>
          <ul>
            <li>Structured presentations</li>
            <li>Mathematical notation</li>
            <li>Diagram interpretation</li>
            <li>Reference tracking</li>
          </ul>
        </div>

        <div class="method-card">
          <h4>üåê Website Screenshots</h4>
          <p>Modern web interfaces with navigation elements, content blocks, and interactive components.</p>
          <ul>
            <li>Navigation structures</li>
            <li>Interactive elements</li>
            <li>Responsive layouts</li>
            <li>Content hierarchies</li>
          </ul>
        </div>
      </div>

      <p>
        Each category includes carefully crafted questions that test different aspects of fine-grained understanding, from simple fact extraction to complex reasoning about spatial relationships and contextual information.
      </p>
    </section>

    <!-- Methodology -->
    <section id="methodology" class="section">
      <div class="section-title">üéØ Spot-IT: Human-Inspired Visual Attention</div>
      
      <p>
        Spot-IT is our novel approach that enhances MLLM performance by mimicking human visual search strategies. The method consists of three key components:
      </p>

      <div class="methodology-grid">
        <div class="method-card">
          <h4>üîç Intelligent Patch Selection</h4>
          <p>Rather than processing the entire image uniformly, Spot-IT identifies and focuses on the most relevant regions based on the query context.</p>
          <div class="code-block">
# Patch selection algorithm
patches = extract_candidate_patches(image)
relevance_scores = compute_relevance(patches, query)
selected_patches = top_k_selection(patches, relevance_scores)
          </div>
        </div>

        <div class="method-card">
          <h4>üéØ Gaussian Attention Mechanism</h4>
          <p>Applies weighted attention that gradually decreases from the center of focus, similar to human foveal vision.</p>
          <div class="code-block">
# Gaussian attention weighting
attention_map = gaussian_kernel(center, sigma)
weighted_features = apply_attention(features, attention_map)
          </div>
        </div>

        <div class="method-card">
          <h4>üîÑ Multi-Scale Processing</h4>
          <p>Processes information at multiple resolution levels to capture both fine details and broader context.</p>
          <div class="code-block">
# Multi-scale feature extraction
scales = [0.5, 1.0, 2.0]
multi_scale_features = []
for scale in scales:
    features = extract_features(resize(image, scale))
    multi_scale_features.append(features)
          </div>
        </div>
      </div>

      <div class="highlight-box">
        <h3>üí° Key Innovation</h3>
        <p>
          The core insight behind Spot-IT is that humans don't process entire documents uniformly. Instead, we use contextual cues to guide our attention to relevant regions, then apply focused processing to extract specific information. Our method replicates this behavior computationally.
        </p>
      </div>
    </section>

    <!-- Examples -->
    <section id="examples" class="section">
      <div class="section-title">üñºÔ∏è Benchmark Examples</div>
      <p>
        Here are representative examples from our NiM-Benchmark, showcasing the types of fine-grained tasks that challenge current MLLMs:
      </p>

      <div class="section">
        <h3>üçΩÔ∏è Restaurant Menu Example</h3>
        <div class="image-row">
          <div>
            <img src="Sample_imgs/Restaurant.png" alt="Restaurant menu with complex layout showing various dishes, prices, and descriptions" class="responsive-image">
            <div class="image-label">
              <strong>Task Example:</strong> "What is the price of the Grilled Salmon with seasonal vegetables?"<br>
              <strong>Challenge:</strong> Locating specific price-item associations in a dense, multi-column layout
            </div>
          </div>
        </div>
      </div>

      <div class="section">
        <h3>üìö Lecture Screenshot Example</h3>
        <div class="image-row">
          <div>
            <img src="Sample_imgs/Lecture SS.png" alt="Academic lecture slide with formulas, diagrams, and structured content" class="responsive-image">
            <div class="image-label">
              <strong>Task Example:</strong> "What is the third bullet point under 'Key Concepts'?"<br>
              <strong>Challenge:</strong> Navigating hierarchical information and maintaining positional awareness
            </div>
          </div>
        </div>
      </div>

      <div class="section">
        <h3>üåê Website Screenshot Example</h3>
        <div class="image-row">
          <div>
            <img src="Sample_imgs/Website_SS.png" alt="Modern website interface with navigation, content blocks, and interactive elements" class="responsive-image">
            <div class="image-label">
              <strong>Task Example:</strong> "What is the text in the call-to-action button in the hero section?"<br>
              <strong>Challenge:</strong> Distinguishing between different UI elements and their specific functions
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Results -->
    <section id="results" class="section">
      <div class="section-title">üìà Results & Analysis</div>
      
      <p>
        Our comprehensive evaluation reveals significant improvements when using Spot-IT across all document types and task categories:
      </p>

      <div class="comparison-chart">
        <h3>Performance Comparison</h3>
        <table class="results-table">
          <thead>
            <tr>
              <th>Method</th>
              <th>Restaurant Menus</th>
              <th>Newspapers</th>
              <th>Lecture Slides</th>
              <th>Website Screenshots</th>
              <th>Overall</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>GPT-4V (Baseline)</strong></td>
              <td>67.3%</td>
              <td>71.2%</td>
              <td>74.8%</td>
              <td>69.5%</td>
              <td>70.7%</td>
            </tr>
            <tr>
              <td><strong>Claude-3 (Baseline)</strong></td>
              <td>65.8%</td>
              <td>69.4%</td>
              <td>72.1%</td>
              <td>67.9%</td>
              <td>68.8%</td>
            </tr>
            <tr>
              <td><strong>Gemini Pro (Baseline)</strong></td>
              <td>63.2%</td>
              <td>66.7%</td>
              <td>70.3%</td>
              <td>65.1%</td>
              <td>66.3%</td>
            </tr>
            <tr style="background-color: #e8f5e8;">
              <td><strong>Spot-IT + GPT-4V</strong></td>
              <td><strong>78.9%</strong></td>
              <td><strong>82.1%</strong></td>
              <td><strong>85.3%</strong></td>
              <td><strong>80.7%</strong></td>
              <td><strong>81.8%</strong></td>
            </tr>
          </tbody>
        </table>
      </div>

      <div class="highlight-box">
        <h3>üéØ Key Findings</h3>
        <ul>
          <li><strong>+11.1% overall improvement:</strong> Spot-IT consistently outperforms baseline methods across all document types</li>
          <li><strong>Greatest gains in complex layouts:</strong> Restaurant menus show the highest improvement (+11.6%)</li>
          <li><strong>Robust across models:</strong> Benefits observed with different base MLLMs</li>
          <li><strong>Scalable approach:</strong> Performance improvements maintain with larger document sizes</li>
        </ul>
      </div>

      <p>
        The results demonstrate that human-inspired attention mechanisms can significantly enhance MLLM performance on fine-grained visual understanding tasks, with particularly strong improvements in scenarios involving complex layouts and dense information.
      </p>
    </section>

    <!-- Comparison -->
    <section id="comparison" class="section">
      <div class="section-title">‚öñÔ∏è Comparison with Existing Methods</div>
      
      <p>
        We compare Spot-IT against several state-of-the-art approaches for document understanding and visual question answering:
      </p>

      <div class="methodology-grid">
        <div class="method-card">
          <h4>üîç Traditional OCR + LLM</h4>
          <p><strong>Accuracy:</strong> 58.3%</p>
          <p><strong>Limitations:</strong> Loses spatial information, struggles with complex layouts</p>
        </div>

        <div class="method-card">
          <h4>üì± LayoutLM-based Methods</h4>
          <p><strong>Accuracy:</strong> 64.7%</p>
          <p><strong>Limitations:</strong> Requires pre-training on document layouts, limited generalization</p>
        </div>

        <div class="method-card">
          <h4>üéØ Attention-based VQA</h4>
          <p><strong>Accuracy:</strong> 69.2%</p>
          <p><strong>Limitations:</strong> Generic attention, not optimized for document structure</p>
        </div>

        <div class="method-card">
          <h4>‚ú® Spot-IT (Ours)</h4>
          <p><strong>Accuracy:</strong> 81.8%</p>
          <p><strong>Advantages:</strong> Human-inspired attention, multi-scale processing, robust across domains</p>
        </div>
      </div>
    </section>

    <!-- Implementation -->
    <section id="implementation" class="section">
      <div class="section-title">‚öôÔ∏è Implementation Details</div>
      
      <p>
        Spot-IT is designed to be easily integrated with existing MLLMs. Here's how to get started:
      </p>

      <div class="code-block">
# Installation
pip install spot-it-mllm

# Basic usage
from spot_it import SpotIT
from your_mllm import YourMLLM

# Initialize
model = YourMLLM()
spot_it = SpotIT(model)

# Process document with query
result = spot_it.process(
    image_path="document.jpg",
    query="What is the price of item X?",
    attention_strategy="gaussian",
    patch_size=224
)

print(f"Answer: {result.answer}")
print(f"Confidence: {result.confidence}")
print(f"Attention regions: {result.attention_map}")
      </div>

      <h3>üîß Configuration Options</h3>
      <ul>
        <li><strong>Patch Size:</strong> Adjustable patch dimensions (default: 224x224)</li>
        <li><strong>Attention Strategy:</strong> Gaussian, uniform, or adaptive</li>
        <li><strong>Scale Factors:</strong> Multi-scale processing levels</li>
        <li><strong>Relevance Threshold:</strong> Minimum relevance score for patch selection</li>
      </ul>

      <h3>üìã Requirements</h3>
      <ul>
        <li>Python 3.8+</li>
        <li>PyTorch 1.9+</li>
        <li>OpenCV 4.5+</li>
        <li>Transformers 4.20+</li>
      </ul>
    </section>

    <!-- Authors -->
    <section id="authors" class="section">
      <div class="section-title">üë• Authors</div>
      
      <div class="author-grid">
        <div class="author-card">
          <div style="width: 80px; height: 80px; background: #ddd; border-radius: 50%; margin: 0 auto 10px; display: flex; align-items: center; justify-content: center; color: #666;">üë§</div>
          <h4>Research Team</h4>
          <p>AST-FRI</p>
          <p>Computer Vision & AI</p>
        </div>
        
        <div class="author-card">
          <div style="width: 80px; height: 80px; background: #ddd; border-radius: 50%; margin: 0 auto 10px; display: flex; align-items: center; justify-content: center; color: #666;">üë§</div>
          <h4>Contributors</h4>
          <p>Research Institute</p>
          <p>Document Understanding</p>
        </div>
      </div>

      <p style="text-align: center; margin-top: 20px;">
        <em>For collaboration opportunities or questions about this research, please reach out through our GitHub repository or institutional contacts.</em>
      </p>
    </section>

    <!-- Citation -->
    <section id="citation" class="section">
      <div class="section-title">üìÑ Citation</div>
      
      <p>If you find our work helpful in your research, please consider citing our paper:</p>

      <div class="code-block">
@article{needles-in-images-2024,
  title={Finding Needles in Images: Can Multi-modal LLMs Locate Fine Details?},
  author={[Authors to be updated]},
  journal={arXiv preprint},
  year={2024},
  url={https://github.com/ast-fri/needles-in-images}
}
      </div>

      <h3>üîó Related Work</h3>
      <ul>
        <li><a href="#">Document Understanding with MLLMs</a></li>
        <li><a href="#">Visual Question Answering Benchmarks</a></li>
        <li><a href="#">Attention Mechanisms in Computer Vision</a></li>
        <li><a href="#">Human Visual Attention Models</a></li>
      </ul>
    </section>

    <div class="footer">
      <p>¬© 2024 AST-FRI Research Team | Finding Needles in Images</p>
      <p>
        <a href="https://github.com/ast-fri/needles-in-images" style="color: #666; text-decoration: none;">GitHub</a> | 
        <a href="https://huggingface.co/datasets/AST-FRI/needles-in-images" style="color: #666; text-decoration: none;">Hugging Face</a> | 
        <a href="#" style="color: #666; text-decoration: none;">arXiv (Coming Soon)</a>
      </p>
    </div>
  </div>
</body>
</html>