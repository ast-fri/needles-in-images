<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Finding Needles in Images: Can Multi-modal LLMs Locate Fine Details?</title>
  <meta name="description" content="A comprehensive benchmark and method for evaluating Multi-modal Large Language Models on fine-grained visual document understanding tasks.">
  <meta name="keywords" content="MLLM, computer vision, document understanding, benchmark, AI, machine learning">
  <meta name="author" content="AST-FRI Research Team">
  <style>
    body {
      margin: 0;
      font-family: 'Segoe UI', Tahoma, sans-serif;
      background-color: #fafafa;
      color: #222;
      line-height: 1.7;
    }

    .container {
      max-width: 960px;
      margin: 0 auto;
      padding: 40px 20px;
    }

    .header {
      text-align: center;
      margin-bottom: 40px;
    }

    .header h1 {
      font-size: 2.4rem;
      font-weight: 700;
      color: #000;
      margin-bottom: 12px;
      border-bottom: 2px solid #ccc;
      display: inline-block;
      padding-bottom: 6px;
    }


    .header p {
      font-size: 1rem;
      color: #555;
    }

    .links-section {
      display: flex;
      justify-content: center;
      gap: 15px;
      flex-wrap: wrap;
      margin-top: 25px;
    }

    .link-button {
      display: inline-flex;
      align-items: center;
      gap: 8px;
      background: #222;
      color: #fff;
      text-decoration: none;
      padding: 10px 20px;
      border-radius: 30px;
      font-size: 0.9rem;
      transition: background 0.2s ease;
    }

    .link-button:hover {
      background: #444;
    }

    .link-button.github {
      background: #24292e;
    }

    .link-button.huggingface {
      background: #ff9900;
    }

    .link-button.arxiv {
      background: #b91c1c;
    }

    .section {
      background: #fff;
      border: 1px solid #e0e0e0;
      border-radius: 8px;
      padding: 30px;
      margin-bottom: 40px;
      box-shadow: 0 2px 5px rgba(0, 0, 0, 0.02);
    }

    .section-title {
      font-size: 1.5rem;
      font-weight: 500;
      margin-bottom: 20px;
      border-bottom: 1px solid #ddd;
      padding-bottom: 10px;
      color: #111;
    }

    .abstract {
      font-size: 1rem;
      color: #333;
      text-align: justify;
    }

    .image-row {
      display: flex;
      flex-wrap: wrap;
      gap: 20px;
      justify-content: center;
      align-items: center;
    }

    .image-row img {
      border-radius: 8px;
      box-shadow: 0 2px 10px rgba(0, 0, 0, 0.05);
      max-width: 100%;
    }

    .image-label {
      text-align: center;
      color: #666;
      font-size: 0.9rem;
      margin-top: 8px;
    }

    .footer {
      text-align: center;
      font-size: 0.9rem;
      color: #888;
      margin-top: 60px;
      padding: 20px;
    }

    .table-of-contents {
      background: #f8f9fa;
      border: 1px solid #e9ecef;
      border-radius: 8px;
      padding: 20px;
      margin-bottom: 30px;
    }

    .table-of-contents h3 {
      margin-top: 0;
      color: #333;
      font-size: 1.2rem;
    }

    .table-of-contents ul {
      list-style: none;
      padding-left: 0;
    }

    .table-of-contents li {
      margin: 8px 0;
    }

    .table-of-contents a {
      color: #007bff;
      text-decoration: none;
      font-weight: 500;
    }

    .table-of-contents a:hover {
      text-decoration: underline;
    }

    .methodology-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 20px;
      margin: 20px 0;
    }

    .method-card {
      background: #f8f9fa;
      border: 1px solid #e9ecef;
      border-radius: 8px;
      padding: 20px;
    }

    .method-card h4 {
      color: #333;
      margin-top: 0;
      font-size: 1.1rem;
    }

    .results-table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
      background: white;
    }

    .results-table th,
    .results-table td {
      border: 1px solid #ddd;
      padding: 12px;
      text-align: left;
    }

    .results-table th {
      background-color: #f8f9fa;
      font-weight: 600;
      color: #333;
    }

    .results-table tr:nth-child(even) {
      background-color: #f9f9f9;
    }

    .highlight-box {
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      color: white;
      padding: 20px;
      border-radius: 8px;
      margin: 20px 0;
    }

    .highlight-box h3 {
      margin-top: 0;
      color: white;
    }

    .code-block {
      background: #f8f9fa;
      border: 1px solid #e9ecef;
      border-radius: 4px;
      padding: 15px;
      font-family: 'Courier New', monospace;
      font-size: 0.9rem;
      overflow-x: auto;
      margin: 15px 0;
    }

    .author-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
      gap: 20px;
      margin: 20px 0;
    }

    .author-card {
      text-align: center;
      padding: 15px;
      background: #f8f9fa;
      border-radius: 8px;
    }

    .author-card img {
      width: 80px;
      height: 80px;
      border-radius: 50%;
      margin-bottom: 10px;
    }

    .comparison-chart {
      background: white;
      border: 1px solid #e0e0e0;
      border-radius: 8px;
      padding: 20px;
      margin: 20px 0;
    }

    @media (max-width: 768px) {
      .image-row {
        flex-direction: column;
      }
      
      .methodology-grid {
        grid-template-columns: 1fr;
      }
      
      .author-grid {
        grid-template-columns: 1fr;
      }
      
      .results-table {
        font-size: 0.8rem;
      }
    }
  </style>
</head>
<body>
  <div class="container">
    <header class="header">
      <h1>Finding Needles in Images: Can Multi-modal LLMs Locate Fine Details?</h1>
      <p>A comprehensive benchmark and method for evaluating Multi-modal Large Language Models on fine-grained visual document understanding tasks.</p>

      <div class="links-section">
        <a href="https://github.com/ast-fri/needles-in-images" class="link-button github">
          <span>ðŸ“„</span> GitHub
        </a>
        <a href="https://huggingface.co/datasets/AST-FRI/needles-in-images" class="link-button huggingface">
          <span>ðŸ¤—</span> Hugging Face
        </a>
        <a href="https://arxiv.org/abs/your-paper-id" class="link-button arxiv">
          <span>ðŸ“š</span> arXiv (Coming Soon)
        </a>
      </div>
    </header>

    <!-- Table of Contents -->
    <div class="table-of-contents">
      <h3>ðŸ“‹ Table of Contents</h3>
      <ul>
        <li><a href="#abstract">Abstract</a></li>
        <li><a href="#introduction">Introduction</a></li>
        <li><a href="#benchmark">NiM-Benchmark</a></li>
        <li><a href="#methodology">Spot-IT Methodology</a></li>
        <li><a href="#examples">Examples</a></li>
        <li><a href="#results">Results & Analysis</a></li>
        <li><a href="#comparison">Comparison with Baselines</a></li>
        <li><a href="#implementation">Implementation Details</a></li>
        <li><a href="#authors">Authors</a></li>
        <li><a href="#citation">Citation</a></li>
      </ul>
    </div>

    <section class="section">
      <div class="section-title">Abstract</div>
      <p class="abstract">
        While Multi-modal Large Language Models (MLLMs) have shown impressive capabilities in document understanding tasks, their ability to locate and reason about fine-grained details within complex documents remains understudied. Consider searching a restaurant menu for a specific nutritional detail or locating a particular warranty clause in a manual â€“ tasks that require precise attention to minute details within a larger context, akin to Finding Needles in Images (NiM). To address this gap, we introduce NiM-Benchmark, a carefully curated benchmark spanning diverse real-world documents including newspapers, menus, and lecture images, specifically designed to evaluate MLLMsâ€™ capability in these intricate tasks. Building on this, we further propose Spot-IT, a simple yet effective approach that enhances MLLMs capability through intelligent patch selection and Gaussian attention, motivated from how humans zoom and focus when searching documents. Our extensive experiments reveal both the capabilities and limitations of current MLLMs in handling fine-grained document understanding tasks, while demonstrating the effectiveness of our approach. Spot-IT achieves significant improvements over baseline methods, particularly in scenarios requiring precise detail extraction from complex layouts.
      </p>
    </section>

    <section class="section">
      <div class="section-title">Restaurant menu example</div>
      <div class="image-row">
        <div>
          <img src="Sample_imgs/Restaurant.png" alt="Full Image" class="responsive-image">
          <div class="image-label">NiM benchmark sample image with Spot-IT(Restaurant Menus)</div>
        </div>
      </div>
      </section>

      <section class="section">
      <div class="section-title">Lecture screenshot example</div>
      <div class="image-row">
        <div>
          <img src="Sample_imgs/Lecture SS.png" alt="Full Image" class="responsive-image">
          <div class="image-label">NiM benchmark sample image with Spot-IT</div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="section-title">Website Screenshot example</div>
      <div class="image-row">
        <div>
          <img src="Sample_imgs/Website_SS.png" alt="Full Image" class="responsive-image">
          <div class="image-label">NiM benchmark sample image with Spot-IT</div>
        </div>
      </div>
    </section>
    

    <div class="footer">
      Â© 2024 Research Project | Finding Needles in Images
    </div>
  </div>
</body>
</html>
